{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vcb = 366549\n",
    "path = '/media/rafael/SSD 1Tb/DADOS/Dataset_SP.csv'\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/rafael/D/2020 RAFAEL/Faculdade/ICs/IC Direito/codes/pre-processing/3. Fine-Tuning NER rodando no servidor/Resultados em toda base de SP/utils/tokenizer_100k.json') as f: \n",
    "    data = json.load(f) \n",
    "    tok = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive(data, stepsize=1):\n",
    "    return list(np.split(data, np.where(np.diff(data) != stepsize)[0]+1))\n",
    "\n",
    "def undersample_data(batch_x, batch_y, Rs):\n",
    "\n",
    "    undersampled_X_seq = [] # new list of undersampled word samples\n",
    "    undersampled_Y_seq = [] # new list of undersampled classes samples\n",
    "    \n",
    "    for sentence,classe in zip(batch_x, batch_y): # for sentence in dataset\n",
    "\n",
    "        unique, counts = np.unique(classe, return_counts=True)\n",
    "\n",
    "        Np = len(classe) - counts[0] # Number of positives tokens\n",
    "        Nn = counts[0] # Number of negative tokens\n",
    "        Ns = 0 # Number of selected negative tokens in sentence\n",
    "        \n",
    "        array_classe = np.array(classe) # just a convert type from list to numpy array\n",
    "        array_sentence = np.array(sentence) # just a convert type from list to numpy array\n",
    "\n",
    "        selected_mask = array_classe > 1 # taking the mask of all positive classes \n",
    "\n",
    "        if(Np != 0): # if the setence has at least a single positive tag \n",
    "            if(Nn/Np <= Rs): #if the Ratio of Negative tokens is lower then our Ratio: no need for undersampling, select all tokens\n",
    "                undersampled_X_seq.append(sentence)\n",
    "                undersampled_Y_seq.append(classe)\n",
    "            else: \n",
    "                while (Ns/Np <= Rs):\n",
    "                    idx_mask = list(np.where(selected_mask == True)) # select the index where our mask is True\n",
    "                    idx_consecutives = consecutive(idx_mask[0]) # group a sequence of array\n",
    "                    # select all in the left first, after all in the right\n",
    "                    for entity in idx_consecutives:\n",
    "                        m_left_el = np.min(entity) # most left element\n",
    "                        try:\n",
    "                            #selecting the most left element not selected in group\n",
    "                            if(not selected_mask[m_left_el - 1]): \n",
    "                                selected_mask[m_left_el - 1] = True\n",
    "                                Ns += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    for entity in idx_consecutives:\n",
    "                        m_right_el = np.max(entity) # most right element\n",
    "                        try:\n",
    "                            #selecting the most left element not selected in group\n",
    "                            if(not selected_mask[m_right_el + 1]): \n",
    "                                selected_mask[m_right_el + 1] = True\n",
    "                                Ns += 1\n",
    "                        except:\n",
    "                            pass\n",
    "            \n",
    "                undersampled_Y_seq.append(list(compress(classe, selected_mask.tolist()))) # add the undersampled sample in new list\n",
    "                undersampled_X_seq.append(list(compress(sentence, selected_mask.tolist()))) # add the undersampled sample in new list\n",
    "\n",
    "    return undersampled_X_seq, undersampled_Y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, path, skiprows):\n",
    "    total = []\n",
    "\n",
    "    with open(path, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "\n",
    "        while True:\n",
    "\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            Palavras = []\n",
    "            Classes = []\n",
    "            Sentenças = []\n",
    "            N_batches = 1\n",
    "\n",
    "            for i, row in enumerate(spamreader):\n",
    "\n",
    "                if (skiprows != 0):\n",
    "                    for line in range(skiprows):\n",
    "                        row.__next__\n",
    "\n",
    "                indices_virgula = [m.start(0) for m in re.finditer(',', row[0])]\n",
    "                row = row[0]\n",
    "\n",
    "                palavra = row[indices_virgula[0]+1: indices_virgula[1]]\n",
    "                classe = int(row[indices_virgula[1]+1: indices_virgula[2]]) + 1\n",
    "                sentença = row[indices_virgula[2]+1: indices_virgula[3]]\n",
    "\n",
    "                if (sentença not in total):\n",
    "                    total.append(sentença)\n",
    "                    \n",
    "                    if (len(total) != 1): # se a lista não é vazia\n",
    "                        \n",
    "                        all_is_1 = [True for clas in Classes[0:] if clas != 1]\n",
    "\n",
    "                        if(len(all_is_1) != 0): # se a sentença possui pelo menos alguma palavra que seja uma classe de interesse\n",
    "\n",
    "                            batch_x.append(Palavras[0:]) # todas as palavras são uma sentença\n",
    "                            batch_y.append(Classes[0:])\n",
    "                            Palavras = [] # logo esvazia o vetor de palavras\n",
    "                            Classes = []\n",
    "                        else:\n",
    "                            Palavras = [] # logo esvazia o vetor de palavras\n",
    "                            Classes = []\n",
    "                            del total[-2]\n",
    "                        \n",
    "\n",
    "                Palavras.append(palavra)\n",
    "                Classes.append(classe)\n",
    "                \n",
    "                if (len(batch_x) == batch_size):\n",
    "\n",
    "                    batch_x = tok.texts_to_sequences(batch_x)\n",
    "                    batch_x, batch_y = undersample_data(batch_x, batch_y, 34)\n",
    "\n",
    "                    max_len=150\n",
    "\n",
    "                    batch_x = pad_sequences(maxlen=max_len,\n",
    "                                    sequences=batch_x,\n",
    "                                    padding=\"post\",\n",
    "                                    truncating=\"post\")\n",
    "\n",
    "                    batch_y = pad_sequences(maxlen=max_len,\n",
    "                                    sequences=batch_y,\n",
    "                                    padding=\"post\",\n",
    "                                    truncating=\"post\")\n",
    "                    total = []\n",
    "\n",
    "                    # produz batch de valores (x,y)\n",
    "                    yield np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "                    batch_x = []\n",
    "                    batch_y = []\n",
    "\n",
    "                    Palavras = []\n",
    "                    Classes = []\n",
    "                    Sentenças = []\n",
    "                    N_batches += 1\n",
    "\n",
    "                    if (N_batches % 5000 == 0):\n",
    "                        f = open(\"saved_line.txt\", \"w\")\n",
    "                        f.write(f'\\n line is: {i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen = data_generator(batch_size, path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.3.0', True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, SpatialDropout1D, Bidirectional, Input, Masking\n",
    "\n",
    "tf.__version__, tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 512 # output da LSTM\n",
    "Emb_dims = 50 # tamanho do vetor de embeddings\n",
    "Spa_dropout = 0.5 # spacial dropout rate\n",
    "n_tags = 18 # quantidade de tags\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(None,)))\n",
    "model.add(Embedding(input_dim=len_vcb + 1, output_dim=Emb_dims, mask_zero=True))\n",
    "model.add(SpatialDropout1D(Spa_dropout))\n",
    "model.add(Bidirectional(LSTM(units=units, return_sequences=True), merge_mode='sum'))\n",
    "model.add(Dense(n_tags, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### definindo metricas customizadas\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_pred = tf.cast(K.argmax(y_pred), tf.float32) # transformando o vetor de (None, 150, n_tags) para (None, 150)\n",
    "    macro_recall = 0\n",
    "    y_pred = y_pred[y_true != 0] # retira zero\n",
    "    y_true = y_true[y_true != 0] # retira zero\n",
    "    equals = y_pred[K.equal(y_true,y_pred)]  # calcula os verdadeiros positivos, ignorando o zero\n",
    "    \n",
    "    for i in range(1,n_tags):\n",
    "        equals_positives = equals[equals == i]  \n",
    "        true_positives = K.sum(equals_positives/i) # divide por i, para que os valores se tornem 1\n",
    "\n",
    "        equals_possibles = y_true[K.equal(y_true,K.constant(i))]/i\n",
    "        possible_positives = K.sum(equals_possibles)\n",
    "\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        macro_recall = macro_recall + recall\n",
    "\n",
    "    macro_recall = macro_recall/(n_tags-1)  \n",
    "\n",
    "    return macro_recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_pred = tf.cast(K.argmax(y_pred), tf.float32) # trannsformando o vetor de (None, 150, n_tags) para (None, 150)\n",
    "    macro_precision = 0\n",
    "    y_pred = y_pred[y_true != 0] # retira zero\n",
    "    y_true = y_true[y_true != 0] # retira zero\n",
    "    equals = y_true[K.equal(y_true,y_pred)]  # calcula os verdadeiros positivos, ignorando o zero\n",
    "    \n",
    "    for i in range(1,n_tags):\n",
    "        equals_positives = equals[equals == i]  \n",
    "        true_positives = K.sum(equals_positives/i) # divide por i, para que os valores se tornem 1\n",
    "\n",
    "        equals_pred_possibles = y_pred[K.equal(y_pred,K.constant(i))]/i\n",
    "        predicted_positives = K.sum(equals_pred_possibles)\n",
    "\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        macro_precision = macro_precision + precision\n",
    "\n",
    "    macro_precision = macro_precision/(n_tags-1)\n",
    "\n",
    "    return macro_precision\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2*((prec*rec)/(prec+rec+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"SparseCategoricalAccuracy\",\n",
    "              recall,\n",
    "              precision,\n",
    "              macro_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='/media/rafael/D/2020 RAFAEL/Faculdade/ICs/IC Direito/codes/pre-processing/3. Fine-Tuning NER rodando no servidor/Resultados em toda base de SP/Callbacks/weights.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='macro_f1',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258213/258212 [==============================] - 141149s 547ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9965 - recall: 0.4673 - precision: 0.4950 - macro_f1: 0.4802\n",
      "O modelo levou 141162.12717437744 s durante o treino.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        model.fit(gen,\n",
    "                  epochs=1,\n",
    "                  steps_per_epoch=66102346/batch_size,\n",
    "                  callbacks=[model_checkpoint_callback])\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "print('O modelo levou {} s durante o treino.'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SP_TOTAL_1EPOCH.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
